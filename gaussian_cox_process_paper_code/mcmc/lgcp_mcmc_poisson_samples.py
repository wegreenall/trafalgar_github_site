import torch
import numpy as np
from gcp_rssb.data import PoissonProcess, PoissonProcess2d
import matplotlib.pyplot as plt
from math import sqrt
from typing import Callable
from mercergp.kernels import SmoothExponentialKernel
from scipy.interpolate import UnivariateSpline, NearestNDInterpolator, interp1d

"""
In order to be able to calculate the EC Criterion,
for the MCMC approach and assuming a log Gaussian Cox process,
we need to generate samples from the posterior predictive
distribution. This is simple in the cases of the other compared methods
given that we have calculated full intensity functions.

However, in the MCMC case  we have to generate samples from essentially a
piecewise continuous intensity function. The posterior samples have been
generated as part of the estimation mprocess in the corresponding lgcp_mcmc.py
and lgcp_mcmc_1d.py scripts. However, these appear to be point samples due to
the piecewise form of the samples as generated by the MCMC process as
implemented in the corresponding file.

In this script, we will complete this task.
"""


class IntensityNearestNeighbour:
    def __init__(
        self, sample: torch.Tensor, sample_inputs: torch.Tensor, dimension: int
    ):
        """
        Represents a Callable intensity function that is capable of representing
        the intensity function of a Poisson process. The intensity function is
        essentially an interpolation of the MCMC sample points.

        It does this using splines polynomials.
        """
        self.sample = sample  # n x 1
        if sample.shape == (sample.shape[0],):
            self.sample = self.sample.unsqueeze(1)

        self.sample_inputs = sample_inputs  # n x d
        self.dimension = dimension  # d

        # set up the spline
        if self.dimension == 1:
            self.interpolator = interp1d(
                self.sample_inputs.squeeze(),
                self.sample.squeeze(),
                kind="previous",
            )
        else:
            self.interpolator = NearestNDInterpolator(
                self.sample_inputs,
                self.sample.flatten(),
            )

    def __call__(self, x: torch.Tensor):
        """
        Given a point x, return the intensity at that point.
        """
        # if flat, expand and set d = 1
        if x.shape == (x.shape[0],):
            x = x.unsqueeze(1)
            d = 1
        else:
            d = x.shape[1]

        N = x.shape[0]

        n = self.sample_inputs.shape[0]
        assert d == self.dimension
        return torch.Tensor([self.interpolator(x.squeeze())]).squeeze()


class IntensitySplineInterpolation:
    def __init__(
        self, sample: torch.Tensor, sample_inputs: torch.Tensor, dimension: int
    ):
        """
        Represents a Callable intensity function that is capable of representing
        the intensity function of a Poisson process. The intensity function is
        essentially an interpolation of the MCMC sample points.

        It does this using splines polynomials.
        """
        self.sample = sample  # n x 1
        if sample.shape == (sample.shape[0],):
            self.sample = self.sample.unsqueeze(1)

        self.sample_inputs = sample_inputs  # n x d
        self.dimension = dimension  # d

        # set up the spline
        degree = 3
        self.spline = UnivariateSpline(
            self.sample_inputs.squeeze(),
            self.sample.squeeze(),
            k=degree,
            s=0,
        )

    def __call__(self, x: torch.Tensor):
        """
        Given a point x, return the intensity at that point.
        """
        # if flat, expand and set d = 1
        if x.shape == (x.shape[0],):
            x = x.unsqueeze(1)
            d = 1
        else:
            d = x.shape[1]

        N = x.shape[0]

        n = self.sample_inputs.shape[0]
        assert d == self.dimension
        return torch.Tensor([self.spline(x.squeeze() / 2)]).squeeze()


class IntensityPiecewiseConstant:
    """
    Something is incorrect here - it doesn't work.
    """

    def __init__(
        self,
        sample: torch.Tensor,
        sample_inputs: torch.Tensor,
        dimension: int,
        # kernel: Callable,
    ):
        """
        Represents a Callable intensity function that is capable of representing
        the intensity function of a Poisson process. The intensity function is
        essentially an interpolation of the MCMC sample points.

        It does this using Lagrange polynomials.
        """
        self.sample = sample  # n x 1
        if sample.shape == (sample.shape[0],):
            self.sample = self.sample.unsqueeze(1)

        self.sample_inputs = sample_inputs  # n x d
        self.dimension = dimension  # d

    def __call__(self, x: torch.Tensor):
        """
        Given a point x, return the intensity at that point.
        """
        # if flat, expand and set d = 1
        if x.shape == (x.shape[0],):
            x = x.unsqueeze(1)
            d = 1
        else:
            d = x.shape[1]

        N = x.shape[0]

        n = self.sample_inputs.shape[0]
        assert d == self.dimension
        # but how to get the piecewise constant function, given the sample?

        reshaped_input = torch.einsum("dNn->Nnd", x.repeat(1, 1, n))
        # print("reshaped_input shape:", reshaped_input.shape)
        reshaped_sample_inputs = self.sample_inputs.unsqueeze(0).repeat(
            N, 1, 1
        )
        diffs = reshaped_sample_inputs - reshaped_input
        abs_diffs = torch.abs(diffs)
        min_diffs = torch.argmin(abs_diffs, axis=1)
        min_diffs_shifted = min_diffs - torch.ones_like(min_diffs)

        return self.sample[min_diffs].squeeze()


class IntensityKernelInterpolation:
    """
    Something is incorrect here - it doesn't work.
    """

    def __init__(
        self,
        sample: torch.Tensor,
        sample_inputs: torch.Tensor,
        dimension: int,
        kernel: Callable,
    ):
        """
        Represents a Callable intensity function that is capable of representing
        the intensity function of a Poisson process. The intensity function is
        essentially an interpolation of the MCMC sample points.

        It does this using a kernel interpolation.
        """
        self.sample = sample  # N x 1
        if sample.shape == (sample.shape[0],):
            self.sample = self.sample.unsqueeze(1)

        self.sample_inputs = sample_inputs  # N x d
        self.dimension = dimension  # d
        self.jitter_parameter = 1e-5
        self.kernel = kernel

    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        left_term = self.kernel(x, self.sample_inputs).t()
        # right_term = torch.inverse(
        # self.kernel(self.sample_inputs, self.sample_inputs)
        # + self.jitter_parameter * torch.eye(self.sample_inputs.shape[0])
        # )
        right_term = self.kernel.kernel_inverse(self.sample_inputs)
        return left_term @ right_term @ self.sample


class IntensityLagrangePolynomials:
    """
    Something is incorrect here - it doesn't work.
    """

    def __init__(
        self, sample: torch.Tensor, sample_inputs: torch.Tensor, dimension: int
    ):
        """
        Represents a Callable intensity function that is capable of representing
        the intensity function of a Poisson process. The intensity function is
        essentially an interpolation of the MCMC sample points.

        It does this using Lagrange polynomials.
        """
        self.sample = sample  # N x 1
        if sample.shape == (sample.shape[0],):
            self.sample = self.sample.unsqueeze(1)

        self.sample_inputs = sample_inputs  # N x d
        self.dimension = dimension  # d

        # check sample inputs shape
        if self.sample_inputs.shape == (self.sample.shape[0],):
            self.sample_inputs = self.sample_inputs.unsqueeze(1)
        elif self.sample_inputs.shape != (
            self.sample.shape[0],
            self.dimension,
        ):
            raise ValueError(
                "Sample inputs should be of shape (N, d) and sample outputs of shape (N, 1)."
            )

        if self.sample.shape != (self.sample_inputs.shape[0], 1):
            raise ValueError(
                "Sample inputs should be of shape (N, d) and sample outputs of shape (N, 1).\
                Current shape: {}".format(
                    sample.shape
                )
            )

        # construct the denominator tensor
        self._setup_denominator_tensor()

    def _setup_denominator_tensor(self):
        """
        Sets up the 'bare' denominator tensor for the Lagrange polynomial.

        This is the tensor with elements (x_i - x_j) for all i, j in the sample
        inputs. then, the diagonal is replaced with 1s.
        """
        N = self.sample_inputs.shape[0]  # N
        Nnn_sample_inputs = self.sample_inputs.unsqueeze(0)
        Nnn_sample_inputs = Nnn_sample_inputs.repeat(N, 1, 1)
        self.Nnn_sample_inputs = Nnn_sample_inputs
        bare_denominator_tensor = (
            torch.einsum("nmd -> mnd", Nnn_sample_inputs) - Nnn_sample_inputs
        )
        for d in range(self.dimension):
            bare_denominator_tensor[:, :, d] = bare_denominator_tensor[
                :, :, d
            ].masked_fill(torch.eye(N, N).bool(), 1)
        self.bare_denominator_tensor = bare_denominator_tensor
        assert self.bare_denominator_tensor.shape == (N, N, self.dimension)

    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        """
        Given a point x, return the intensity at that point.
        """
        # if flat, expand and set d = 1
        if x.shape == (x.shape[0],):
            x = x.unsqueeze(1)
            d = 1
        else:
            d = x.shape[1]

        N = x.shape[0]

        n = self.sample_inputs.shape[0]
        assert d == self.dimension
        # get the Lagrange polynomial at the given point
        if self.dimension == 1 or self.dimension == 2:
            # numerator
            Nnn_input = x.unsqueeze(2).unsqueeze(
                3
            )  # now expand it to the right shape
            Nnn_input = Nnn_input.repeat(1, 1, n, n)  # N, d, n, n
            Nnn_input = torch.einsum("Ndnm -> Nnmd", Nnn_input)  # N, n, n, d

            reshaped_sample = self.Nnn_sample_inputs.unsqueeze(0).repeat(
                N, 1, 1, 1
            )

            # build out the full repeated sample
            numerator_tensor = Nnn_input - reshaped_sample

            # now replace the diagonal of the tensor...
            for d in range(self.dimension):
                numerator_tensor[:, :, :, d] = numerator_tensor[
                    :, :, :, d
                ].masked_fill(torch.eye(n, n).bool(), 1)
            denominator_tensor = self.bare_denominator_tensor.repeat(
                N, 1, 1, 1
            )
            lagrange = numerator_tensor / denominator_tensor
            lagrange = torch.prod(lagrange, axis=-2)  # should be Nn

            if self.dimension == 2:
                sample = self.sample.unsqueeze(1).repeat(1, self.dimension)
            else:
                sample = self.sample
            polynomial = torch.einsum("Nnd, nd -> Nd", lagrange, sample)
            if self.dimension == 1:
                return polynomial
            else:
                return torch.prod(polynomial, axis=-1).unsqueeze(1)

        else:
            raise ValueError(
                "Only 1D and 2D Poisson process models supported."
            )


class MCMCPoissonSampler:
    def __init__(
        self,
        mcmc_samples: np.ndarray,
        mcmc_sample_inputs: np.ndarray,
        domain: torch.Tensor,
        dimension: int,
    ):
        # the mcmc samples are generated as numpy arrays by the
        # relevant library (pymc3).
        # first, shape the samples
        mcmc_samples = torch.Tensor(mcmc_samples)
        mcmc_sample_inputs = torch.Tensor(mcmc_sample_inputs)
        assert mcmc_sample_inputs.shape == (mcmc_samples.shape[2], dimension)
        # reshape the samples - they should be N x 1 and N x d respectively
        if dimension == 1:
            mcmc_samples = mcmc_samples.reshape(
                mcmc_samples.shape[0] * mcmc_samples.shape[1], -1
            )

            self.mcmc_samples = mcmc_samples
            self.mcmc_sample_inputs = mcmc_sample_inputs
            self.mean_posterior_sample = torch.mean(mcmc_samples, axis=0)
        else:
            mcmc_samples = mcmc_samples.reshape(
                mcmc_samples.shape[0] * mcmc_samples.shape[1],
                int(sqrt(mcmc_samples.shape[2])),
                int(sqrt(mcmc_samples.shape[2])),
            )
            # use meshgrid to get 2-d sample
            self.mcmc_samples = mcmc_samples
            self.mcmc_sample_inputs = mcmc_sample_inputs
            self.mean_posterior_sample = torch.mean(mcmc_samples, axis=0)

        # get an intensity callable given the mcmc_samples

        self.domain = domain
        self.dimension = dimension
        self.poisson_processes = []

        # build the intensity
        for i, sample in enumerate(self.mcmc_samples[:100]):
            intensity = IntensityNearestNeighbour(
                sample,
                torch.Tensor(self.mcmc_sample_inputs),
                self.dimension,
            )
            # if i == 0:
            # plt.plot(
            # torch.linspace(self.domain[0, 0], self.domain[0, 1], 100),
            # intensity(
            # torch.linspace(
            # self.domain[0, 0], self.domain[0, 1], 100
            # )
            # ),
            # )
            # plt.show()

            # build the Poisson process
            if self.dimension == 1:
                self.poisson_processes.append(
                    PoissonProcess(
                        intensity,
                        domain[0, 1],
                        bound=torch.max(sample),
                    )
                )
            elif self.dimension == 2:
                self.poisson_processes.append(
                    PoissonProcess2d(
                        intensity,
                        domain=domain,
                        bound=torch.max(sample),
                    )
                )
            else:
                raise ValueError(
                    "Only 1D and 2D Poisson process models supported."
                )

        # mean_sample = torch.mean(self.mcmc_samples, axis=0)
        # kernel_args = {
        # "ard_parameter": torch.Tensor([[1.0]]),
        # "variance_parameter": torch.Tensor([[1.0]]),
        # "noise_parameter": torch.Tensor([[1e-2]]),
        # }
        # kernel = SmoothExponentialKernel(kernel_args)
        # self.intensity = IntensityKernelInterpolation(
        # mean_sample,
        # torch.Tensor(self.mcmc_sample_inputs),
        # self.dimension,
        # kernel,
        # )

        print("About to plot the learnt intensity...")

        print("Relevant domain:", domain)
        # graph = self.intensity(x_axis).detach().numpy()
        # plt.plot(x_axis, graph)
        # plt.scatter(x_axis, mean_sample)
        # plt.legend(["Intensity", "Mean Posterior Sample"])
        # plt.show()

    def generate_posterior_predictive_samples(self) -> torch.Tensor:
        samples = []
        for i, pp in enumerate(self.poisson_processes):
            print("On pp:", i)
            pp.simulate()
            samples.append(pp.get_data())
        return samples


if __name__ == "__main__":
    samples_names_list = [
        "intensity_samples_mcmc_1d_synth_1",
        "intensity_samples_mcmc_1d_synth_2",
        "intensity_samples_mcmc_1d_synth_3",
        "intensity_samples_mcmc_redwood_full",
        "intensity_samples_mcmc_white_oak",
    ]
    domains = [
        torch.Tensor([[0, 50]]),
        torch.Tensor([[0, 5]]),
        torch.Tensor([[0, 100]]),
        torch.Tensor([[0, 1], [0, 1]]),
        torch.Tensor([[0, 1], [0, 1]]),
    ]
    dimensions = [1, 1, 1, 2, 2]
    # dimensions = [1, 1, 1]
    for sample_name, domain, dimension, i in zip(
        samples_names_list, domains, dimensions, range(len(samples_names_list))
    ):
        posterior_sample = np.load(sample_name + ".npy")
        posterior_sample_inputs = np.load(sample_name + "_inputs.npy")
        # posterior_sample_inputs = torch.linspace(
        # domain[0, 0], domain[0, 1], 70
        # ).unsqueeze(1)
        sample_mean = np.mean(posterior_sample[0], axis=0)
        if dimension == 1:
            sample_mean = sample_mean.squeeze()
            x_axis = torch.linspace(
                domain[0, 0], domain[0, 1], sample_mean.shape[0]
            )
            # plt.legend("Posterior Sample Mean")
            # plt.plot(x_axis, sample_mean)
            # plt.show()
        print(sample_name, end=" shape : ")
        print(posterior_sample.shape)

        sampler = MCMCPoissonSampler(
            posterior_sample, posterior_sample_inputs, domain, dimension
        )
        samples = sampler.generate_posterior_predictive_samples()
        data_loc = "/home/william/phd/programming_projects/gcp_rssb/datasets/comparison_experiments/"
        for j, sample in enumerate(samples):
            if dimension == 1:
                save_name = "mcmc_synth{}_{}.pt".format(i + 1, j + 1)
            else:
                if i == 3:
                    save_name = "data_and_samples/samples_update/samples2D/mcmc_redwood_{}.pt".format(
                        j + 1
                    )
                else:
                    save_name = "data_and_samples/samples_update/samples2D/mcmc_whiteoak_{}.pt".format(
                        j + 1
                    )
            torch.save(sample, data_loc + save_name)

        # plt.hist(samples.numpy(), bins=50)
        # plt.show()
